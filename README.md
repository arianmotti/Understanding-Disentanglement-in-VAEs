# Understanding-Disentanglement-in-VAEs
Exploring disentangled latent representations in Variational Autoencoders using the dSprites dataset
# ðŸ§  Disentanglement in Variational Autoencoders (VAE & Î²-VAE)

This repository contains the implementation and analysis of **Variational Autoencoders (VAE)** and their extension **Î²-VAE**, focusing on disentangled representation learning using the **dSprites** dataset.  
All experiments, code, and results correspond to *Homework 1 of Deep Generative Models (DGM)*.

---

## ðŸ“˜ Overview

The project investigates how the Î² parameter in the VAE objective influences **disentanglement** of the latent space.  
We train several Î²-VAE models, compare their reconstruction quality and KL divergence, and evaluate disentanglement using the **Mutual Information Gap (MIG)** metric.

Main objectives:
- Implement and train **Î²-VAE** with different Î² values (1, 3, 10).  
- Measure **MIG** to quantify disentanglement.  
- Visualize **latent traversals** to interpret learned factors of variation.

---

## ðŸ§© Dataset

**dSprites (Matthey et al., DeepMind 2017)**  
- 2D shapes procedurally generated from six independent ground-truth factors.  
- Each image: `64Ã—64` grayscale binary image.  
- Latent variables:  
  - Shape âˆˆ {square, ellipse, heart}  
  - Scale âˆˆ [0.5, 1]  
  - Orientation âˆˆ [0, 2Ï€]  
  - posX, posY âˆˆ [0, 1]  

---

## âš™ï¸ Model Architecture

| Encoder | Decoder |
|----------|----------|
| Conv2d(1, 32) â†’ ReLU | Linear(h_dim, 8192) |
| Conv2d(32, 64) â†’ ReLU | Conv2dT(128, 64) |
| Conv2d(64, 128) â†’ ReLU | Conv2dT(64, 32) |
| Flatten â†’ Linear(8192, h_dim) | Conv2dT(32, 1) â†’ Sigmoid |

- Latent dimension `h_dim = 128`  
- Optimizer : Adam (lr = 0.001)  
- Training : 50 epochs on dSprites  

---

## ðŸ§® Training Objective

\[
\mathcal{L} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \beta D_{KL}(q_\phi(z|x)\,\|\,p(z))
\]

Î² controls the trade-off between reconstruction accuracy and latent factor independence.  
- **Î² = 1** â†’ Standard VAE  
- **Î² > 1** â†’ Encourages disentanglement through stronger regularization  

---

## ðŸ“Š Results

| Î² | Description | MIG |
|---|--------------|-----|
| 1 | Standard VAE | 0.0108 |
| 3 | Moderate disentanglement | **0.0198** |
| 10 | Over-regularized | 0.0138 |

### Factor-wise MIG

| Factor | Î² = 1 | Î² = 3 | Î² = 10 |
|---------|--------|--------|---------|
| shape | 0.000 | 0.000 | 0.000 |
| scale | 0.000 | 0.000 | 0.000 |
| orientation | 0.0013 | 0.0010 | 0.0000 |
| posX | 0.0195 | **0.0502** | 0.0492 |
| posY | 0.0331 | **0.0480** | 0.0196 |

---

## ðŸŽ¨ Latent Traversal

Latent traversals show how changing a single latent dimension affects generated images:

```python
latent_traversal(model_b10, device, dim=0)
